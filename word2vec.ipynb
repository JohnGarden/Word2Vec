{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3710e01b",
   "metadata": {},
   "source": [
    "# Conhecendo  o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "artigo_treino = pd.read_csv('treino.csv')\n",
    "artigo_teste = pd.read_csv('teste.csv')\n",
    "artigo_treino.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_teste.iloc[643].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6624a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texto = [\n",
    "        \"tenha um bom dia\",\n",
    "        \"tenha um péssimo dia\",\n",
    "        \"tenha um ótimo dia\",\n",
    "        \"tenha um dia ruim\"\n",
    "]\n",
    "\n",
    "vetorizador = CountVectorizer()\n",
    "vetorizador.fit(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vetorizador.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe121e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor_bom = vetorizador.transform([\"bom\"])\n",
    "print(vetor_bom.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor_otimo = vetorizador.transform([\"ótimo\"])\n",
    "print(vetor_otimo.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b51e0",
   "metadata": {},
   "source": [
    "# Link para o arquivo \n",
    "\n",
    "http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cbow_s300.txt\") as f:\n",
    "    for linha in range(10):\n",
    "        print(next(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90336b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação da biblioteca gensim\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4851a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "modelo = KeyedVectors.load_word2vec_format(\"cbow_s300.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.most_similar(\"china\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cacfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.most_similar(\"itália\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.most_similar(positive=[\"brasil\", \"argentina\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c183bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuvens -> nuvem : estrelas -> estrela\n",
    "# nuvens - nuvem + estrela = estrelas\n",
    "\n",
    "modelo.most_similar(positive=[\"nuvens\", \"estrela\"], negative=[\"nuvem\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vieses de gênero -> https://www.weforum.org/stories/2019/01/ai-isn-t-dangerous-but-human-bias-is/\n",
    "modelo.most_similar(positive=[\"médico\", \"mulher\"], negative=[\"homem\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb58c97",
   "metadata": {},
   "source": [
    "## Vetorização de texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163238fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_treino.title.loc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce71ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalação da biblioteca nltk\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo o tokenizador\n",
    "import nltk\n",
    "import string\n",
    "# baixar o modelo punkt (tokenizer) se necessário\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "def tokenizador(texto):\n",
    "    texto = texto.lower()\n",
    "    lista_alfanumerico = []\n",
    "    \n",
    "    for token_valido in nltk.word_tokenize(texto):\n",
    "        if token_valido in string.punctuation:\n",
    "            continue\n",
    "        # mantém tokens que não são apenas pontuação (inclui números e palavras)\n",
    "        lista_alfanumerico.append(token_valido)\n",
    "\n",
    "    return lista_alfanumerico\n",
    "\n",
    "tokenizador(\"TExto Exemplo, 1234.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinação de vetores por soma\n",
    "import numpy as np\n",
    "def combinacao_vetores_por_soma(palavras_numeros):\n",
    "    vetor_resultante = np.zeros(300,)  # vetor de zeros com dimensão 300\n",
    "    for pn in palavras_numeros:\n",
    "        try:\n",
    "            vetor_resultante += modelo.get_vector(pn)\n",
    "        except KeyError:\n",
    "            if pn.isnumeric():\n",
    "                pn = \"0\"*len(pn)  # substitui números por \"0\", \"00\", \"000\", etc.\n",
    "                vetor_resultante += modelo.get_vector(pn)\n",
    "            else:\n",
    "                vetor_resultante += modelo.get_vector(\"unknown\")     \n",
    "    \n",
    "    return vetor_resultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea49032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_numeros = tokenizador(\"texto exemplo 123 callll\")\n",
    "vetor_texto = combinacao_vetores_por_soma(palavras_numeros)\n",
    "print(len(vetor_texto))\n",
    "print(vetor_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecaa1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar função para matrix de vetores\n",
    "def matriz_vetores(textos):\n",
    "    x = len(textos)\n",
    "    y = 300\n",
    "    matriz = np.zeros((x, y))\n",
    "\n",
    "    for i in range(x):\n",
    "        palavras_numeros = tokenizador(textos.iloc[i])\n",
    "        matriz[i] = combinacao_vetores_por_soma(palavras_numeros)\n",
    "\n",
    "    return matriz\n",
    "\n",
    "matriz_vetores_treino = matriz_vetores(artigo_treino.title)\n",
    "matriz_vetores_teste = matriz_vetores(artigo_teste.title)\n",
    "print(matriz_vetores_treino.shape)\n",
    "print(matriz_vetores_teste.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(max_iter=200)\n",
    "LR.fit(matriz_vetores_treino, artigo_treino.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.score(matriz_vetores_teste, artigo_teste.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f792b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_teste.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "label_prevista = LR.predict(matriz_vetores_teste)\n",
    "\n",
    "CR = classification_report(artigo_teste.category, label_prevista)\n",
    "print(CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0850d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "DC = DummyClassifier()\n",
    "DC.fit(matriz_vetores_treino, artigo_treino.category)\n",
    "label_prevista_dc = DC.predict(matriz_vetores_teste)\n",
    "\n",
    "CR_dummy = classification_report(artigo_teste.category, label_prevista_dc)\n",
    "print(CR_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando o modelo com skip-gram\n",
    "# http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc#:~:text=stemmiza%C3%A7%C3%A3o%20e%20outras.-,Word2Vec,-Modelo\n",
    "\n",
    "modelo_skipgram = KeyedVectors.load_word2vec_format(\"skip_s300.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5eaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combinação de vetores por soma\n",
    "import numpy as np\n",
    "def combinacao_vetores_por_soma_skipgram(palavras_numeros):\n",
    "    vetor_resultante = np.zeros(300,)  # vetor de zeros com dimensão 300\n",
    "    for pn in palavras_numeros:\n",
    "        try:\n",
    "            vetor_resultante += modelo_skipgram.get_vector(pn)\n",
    "        except KeyError:\n",
    "            if pn.isnumeric():\n",
    "                pn = \"0\"*len(pn)  # substitui números por \"0\", \"00\", \"000\", etc.\n",
    "                vetor_resultante += modelo_skipgram.get_vector(pn)\n",
    "            else:\n",
    "                vetor_resultante += modelo.get_vector(\"unknown\")     \n",
    "    \n",
    "    return vetor_resultante\n",
    "\n",
    "\n",
    "# Criar função para matrix de vetores\n",
    "def matriz_vetores_skipgram(textos):\n",
    "    x = len(textos)\n",
    "    y = 300\n",
    "    matriz = np.zeros((x, y))\n",
    "\n",
    "    for i in range(x):\n",
    "        palavras_numeros = tokenizador(textos.iloc[i])\n",
    "        matriz[i] = combinacao_vetores_por_soma_skipgram(palavras_numeros)\n",
    "\n",
    "    return matriz\n",
    "\n",
    "matriz_vetores_treino_skipgram = matriz_vetores_skipgram(artigo_treino.title)\n",
    "matriz_vetores_teste_skipgram = matriz_vetores_skipgram(artigo_teste.title)\n",
    "\n",
    "# Previsão com skip-gram\n",
    "LR_skipgram.fit(matriz_vetores_treino_skipgram, artigo_treino.category)\n",
    "label_prevista_skipgram = LR_skipgram.predict(matriz_vetores_teste_skipgram) \n",
    "CR_skipgram = classification_report(artigo_teste.category, label_prevista_skipgram)\n",
    "print(CR_skipgram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994321e",
   "metadata": {},
   "source": [
    "# Utilizando o spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6ebd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino = pd.read_csv('treino.csv')\n",
    "dados_treino.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f382778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando o spaCy\n",
    "!pip install -U spacy\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitetura, centralizada em duas estruturas de dados principais Doc e Vocab. \n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e46e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"Rio de janeiro é  as !@# 2131321 cidade maravilhosa.\"\n",
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce606a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOC -> Conjunto de tokens\n",
    "print(type(doc))\n",
    "\n",
    "# Conhecendo o DOC\n",
    "print(doc[0])\n",
    "print(doc.ents)\n",
    "print(doc[0].is_stop)\n",
    "print(doc[1].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_para_tratamento = (titulos.lower() for titulos in dados_treino[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_textos(doc):\n",
    "    tokens_validos = []\n",
    "    for token in doc:\n",
    "        e_valido = not token.is_stop and token.is_alpha\n",
    "        if e_valido:\n",
    "            tokens_validos.append(token.text)\n",
    "    if len(tokens_validos) > 2:\n",
    "        return \" \".join(tokens_validos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trata_textos(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec784d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "\n",
    "t0 = time()\n",
    "textos_tratados = [trata_textos(doc) for doc in nlp.pipe(textos_para_tratamento,\n",
    "                                                        batch_size=1000,\n",
    "                                                        n_process=-1 #utiliza todos os núcleos do processador\n",
    "                                                         )]\n",
    "\n",
    "tf = (time() - t0)/60\n",
    "print(f\"Tempo de processamento: {tf:.2f} minutos\")                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fc798",
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos_tratados = pd.DataFrame({\n",
    "    \"titulo\": textos_tratados\n",
    "})\n",
    "titulos_tratados.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_modelo = Word2Vec(sg = 0, #CBOW\n",
    "                      window = 2,\n",
    "                      vector_size = 300,\n",
    "                      min_count = 5,\n",
    "                      alpha = 0.03,\n",
    "                      min_alpha = 0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(titulos_tratados))\n",
    "\n",
    "titulos_tratados = titulos_tratados.dropna().drop_duplicates()\n",
    "\n",
    "print(len(titulos_tratados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_lista_tokens = [titulo.split(\" \") for titulo in titulos_tratados.titulo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : - %(message)s\", level=logging.INFO)\n",
    "\n",
    "w2v_modelo.build_vocab(lista_lista_tokens, progress_per=5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(w2v_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.train(lista_lista_tokens, \n",
    "                 total_examples=w2v_modelo.corpus_count,\n",
    "                 epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2cddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.wv.most_similar(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e54075",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.wv.most_similar(\"messi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo Skip-gram\n",
    "w2v_modelo_sg = Word2Vec(sg = 1, # Skip-gram\n",
    "                      window = 5,\n",
    "                      vector_size = 300,\n",
    "                      min_count = 5,\n",
    "                      alpha = 0.03,\n",
    "                      min_alpha = 0.007)\n",
    "\n",
    "w2v_modelo_sg.build_vocab(lista_lista_tokens, progress_per=5000)\n",
    "\n",
    "w2v_modelo_sg.train(lista_lista_tokens, \n",
    "                 total_examples=w2v_modelo_sg.corpus_count,\n",
    "                 epochs=30)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo_sg.wv.most_similar(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fb02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.wv.most_similar(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo_sg.wv.most_similar(\"gm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9cf1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.wv.most_similar(\"gm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.wv.save_word2vec_format(\"modelos/modelo_cbow.txt\", binary=False)\n",
    "w2v_modelo_sg.wv.save_word2vec_format(\"modelos/modelo_skipgram.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60232180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar função para matrix de vetores\n",
    "def matriz_vetores(textos):\n",
    "    x = len(textos)\n",
    "    y = 300\n",
    "    matriz = np.zeros((x, y))\n",
    "\n",
    "    for i in range(x):\n",
    "        palavras_numeros = tokenizador(textos.iloc[i])\n",
    "        matriz[i] = combinacao_vetores_por_soma(palavras_numeros)\n",
    "\n",
    "    return matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5900b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "w2v_modelo_cbow = KeyedVectors.load_word2vec_format(\"modelos/modelo_cbow.txt\")\n",
    "w2v_modelo_sg   = KeyedVectors.load_word2vec_format(\"modelos/modelo_skipgram.txt\")\n",
    "artigo_treino = pd.read_csv('treino.csv')\n",
    "artigo_teste = pd.read_csv('teste.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b920b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['texto', 'pedaço']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\", disable=[\"parser\", \"ner\", \"tagger\", \"textcat\"])\n",
    "\n",
    "def tokenizador(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens_validos = []\n",
    "    for token in doc:\n",
    "        e_valido = not token.is_stop and token.is_alpha\n",
    "        if e_valido:\n",
    "            tokens_validos.append(token.text.lower())    \n",
    "    return tokens_validos\n",
    "\n",
    "texto = \"TExto Exemplo, 1234. mais uma pedaço!!!\"\n",
    "tokens = tokenizador(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c968e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combinação de vetores por soma\n",
    "def combinacao_vetores_por_soma(palavras, modelo):\n",
    "    vetor_resultante = np.zeros((1,300))  # vetor de zeros com dimensão 300\n",
    "    for pn in palavras:\n",
    "        try:\n",
    "            vetor_resultante += modelo.get_vector(pn)\n",
    "        except KeyError:\n",
    "            pass    \n",
    "    \n",
    "    return vetor_resultante\n",
    "\n",
    "vetor_texto = combinacao_vetores_por_soma(tokens, w2v_modelo_cbow)\n",
    "print(vetor_texto.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28a666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 300)\n",
      "(20513, 300)\n"
     ]
    }
   ],
   "source": [
    "# Criar função para matrix de vetores\n",
    "def matriz_vetores(textos, modelo):\n",
    "    x = len(textos)\n",
    "    y = 300\n",
    "    matriz = np.zeros((x, y))\n",
    "\n",
    "    for i in range(x):\n",
    "        palavras = tokenizador(textos.iloc[i])\n",
    "        matriz[i] = combinacao_vetores_por_soma(palavras, modelo)\n",
    "\n",
    "    return matriz\n",
    "\n",
    "matriz_vetores_treino_cbow = matriz_vetores(artigo_treino.title, w2v_modelo_cbow)\n",
    "matriz_vetores_teste_cbow = matriz_vetores(artigo_teste.title, w2v_modelo_cbow)\n",
    "\n",
    "print(matriz_vetores_treino_cbow.shape)\n",
    "print(matriz_vetores_teste_cbow.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8755031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.80      0.71      0.75      6103\n",
      "   cotidiano       0.63      0.80      0.71      1698\n",
      "     esporte       0.93      0.86      0.89      4663\n",
      "   ilustrada       0.13      0.83      0.22       131\n",
      "     mercado       0.84      0.78      0.80      5867\n",
      "       mundo       0.74      0.83      0.79      2051\n",
      "\n",
      "    accuracy                           0.78     20513\n",
      "   macro avg       0.68      0.80      0.69     20513\n",
      "weighted avg       0.82      0.78      0.80     20513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def classficador(modelo, x_treino, y_treino, x_teste, y_teste):\n",
    "    LR = LogisticRegression(max_iter=800)\n",
    "    LR.fit(x_treino, y_treino)\n",
    "    label_prevista = LR.predict(x_teste)\n",
    "    CR = classification_report(y_teste, label_prevista)\n",
    "    print(CR)\n",
    "    return LR\n",
    "\n",
    "LR_cbow = classficador(w2v_modelo_cbow, \n",
    "                       matriz_vetores_treino_cbow,\n",
    "                       artigo_treino.category,\n",
    "                       matriz_vetores_teste_cbow,\n",
    "                       artigo_teste.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f1e149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.81      0.71      0.76      6103\n",
      "   cotidiano       0.64      0.81      0.71      1698\n",
      "     esporte       0.93      0.88      0.90      4663\n",
      "   ilustrada       0.14      0.89      0.25       131\n",
      "     mercado       0.84      0.79      0.82      5867\n",
      "       mundo       0.76      0.84      0.80      2051\n",
      "\n",
      "    accuracy                           0.79     20513\n",
      "   macro avg       0.69      0.82      0.71     20513\n",
      "weighted avg       0.82      0.79      0.80     20513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matriz_vetores_treino_sg = matriz_vetores(artigo_treino.title, w2v_modelo_sg)\n",
    "matriz_vetores_teste_sg = matriz_vetores(artigo_teste.title, w2v_modelo_sg)\n",
    "\n",
    "LR_sg = classficador(w2v_modelo_sg, \n",
    "                       matriz_vetores_treino_sg,\n",
    "                       artigo_treino.category,\n",
    "                       matriz_vetores_teste_sg,\n",
    "                       artigo_teste.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd07c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"modelos/modelo_logistico_cbow.pkl\", \"wb\") as f:\n",
    "    pickle.dump(LR_cbow, f)\n",
    "\n",
    "with open(\"modelos/modelo_logistico_sg.pkl\", \"wb\") as f:\n",
    "    pickle.dump(LR_sg, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
